"""
çŸ¥è¯†åº“ç”Ÿæˆæ¨¡å—

æ ¹æ®åˆ†æç»“æœç”Ÿæˆ Markdown æ ¼å¼çš„çŸ¥è¯†åº“æ–‡æ¡£
"""

import os
import logging
from typing import Dict, List
from pathlib import Path
from datetime import datetime


class KnowledgeBaseGenerator:
    """çŸ¥è¯†åº“ç”Ÿæˆå™¨"""
    
    def __init__(self, config: Dict, output_dir: str = "output"):
        """
        åˆå§‹åŒ–çŸ¥è¯†åº“ç”Ÿæˆå™¨
        
        å‚æ•°:
            config: é…ç½®å­—å…¸
            output_dir: è¾“å‡ºç›®å½•
        """
        self.config = config
        self.output_dir = Path(output_dir)
        self.logger = logging.getLogger(__name__)
        
        # çŸ¥è¯†åº“é…ç½®
        kb_config = config.get("knowledge_base", {})
        self.include_video_details = kb_config.get("include_video_details", True)
        self.generate_wordcloud = kb_config.get("generate_wordcloud", True)
    
    def generate(self, channel_name: str, summary: Dict, analysis_results: List[Dict], videos_data: List[Dict]) -> str:
        """
        ç”Ÿæˆå®Œæ•´çš„çŸ¥è¯†åº“
        
        å‚æ•°:
            channel_name: é¢‘é“åç§°
            summary: é£æ ¼æ€»ç»“
            analysis_results: è§†é¢‘åˆ†æç»“æœåˆ—è¡¨
            videos_data: åŸå§‹è§†é¢‘æ•°æ®åˆ—è¡¨
            
        è¿”å›:
            è¾“å‡ºç›®å½•è·¯å¾„
        """
        self.logger.info(f"å¼€å§‹ç”ŸæˆçŸ¥è¯†åº“: {channel_name}")
        
        # åˆ›å»ºé¢‘é“è¾“å‡ºç›®å½•
        channel_dir = self.output_dir / self._sanitize_filename(channel_name)
        channel_dir.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆæ€»ç»“æ–‡æ¡£
        summary_file = self._generate_summary_doc(channel_dir, channel_name, summary)
        self.logger.info(f"å·²ç”Ÿæˆæ€»ç»“æ–‡æ¡£: {summary_file}")
        
        # ç”Ÿæˆç»Ÿè®¡æ•°æ®æ–‡æ¡£
        stats_file = self._generate_statistics_doc(channel_dir, channel_name, summary, analysis_results)
        self.logger.info(f"å·²ç”Ÿæˆç»Ÿè®¡æ–‡æ¡£: {stats_file}")
        
        # ç”Ÿæˆè§†é¢‘è¯¦æƒ…æ–‡æ¡£
        if self.include_video_details:
            videos_dir = channel_dir / "videos"
            videos_dir.mkdir(exist_ok=True)
            self._generate_video_details(videos_dir, analysis_results, videos_data)
            self.logger.info(f"å·²ç”Ÿæˆè§†é¢‘è¯¦æƒ…æ–‡æ¡£: {videos_dir}")
        
        # ç”Ÿæˆå­¦ä¹ æŒ‡å—
        guide_file = self._generate_learning_guide(channel_dir, channel_name, summary)
        self.logger.info(f"å·²ç”Ÿæˆå­¦ä¹ æŒ‡å—: {guide_file}")
        
        # ç”Ÿæˆè¯äº‘(å¦‚æœé…ç½®å¯ç”¨)
        if self.generate_wordcloud:
            try:
                wordcloud_file = self._generate_wordcloud_image(channel_dir, summary)
                if wordcloud_file:
                    self.logger.info(f"å·²ç”Ÿæˆè¯äº‘å›¾: {wordcloud_file}")
            except Exception as e:
                self.logger.warning(f"ç”Ÿæˆè¯äº‘å¤±è´¥: {str(e)}")
        
        self.logger.info(f"çŸ¥è¯†åº“ç”Ÿæˆå®Œæˆ: {channel_dir}")
        return str(channel_dir)
    
    def _sanitize_filename(self, filename: str) -> str:
        """
        æ¸…ç†æ–‡ä»¶å,ç§»é™¤ä¸åˆæ³•å­—ç¬¦
        
        å‚æ•°:
            filename: åŸå§‹æ–‡ä»¶å
            
        è¿”å›:
            æ¸…ç†åçš„æ–‡ä»¶å
        """
        import re
        # ç§»é™¤ä¸åˆæ³•å­—ç¬¦
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        return filename
    
    def _generate_summary_doc(self, output_dir: Path, channel_name: str, summary: Dict) -> str:
        """
        ç”Ÿæˆæ€»ç»“æ–‡æ¡£
        
        å‚æ•°:
            output_dir: è¾“å‡ºç›®å½•
            channel_name: é¢‘é“åç§°
            summary: é£æ ¼æ€»ç»“
            
        è¿”å›:
            æ–‡æ¡£æ–‡ä»¶è·¯å¾„
        """
        summary_file = output_dir / "summary.md"
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            # æ ‡é¢˜
            f.write(f"# {channel_name} - é¢‘é“é£æ ¼åˆ†ææ€»ç»“\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write("---\n\n")
            
            # åŸºæœ¬ä¿¡æ¯
            f.write("## ğŸ“Š åŸºæœ¬ä¿¡æ¯\n\n")
            f.write(f"- **åˆ†æè§†é¢‘æ€»æ•°**: {summary.get('total_videos', 0)}\n")
            f.write(f"- **æˆåŠŸåˆ†ææ•°é‡**: {summary.get('analyzed_videos', 0)}\n")
            f.write(f"- **ä¸»è¦å†…å®¹ç±»å‹**: {summary.get('primary_type', 'æœªçŸ¥')}\n")
            f.write(f"- **ä¸»è¦ç›®æ ‡å—ä¼—**: {summary.get('primary_audience', 'å¤§ä¼—')}\n\n")
            
            # è§†é¢‘ç±»å‹åˆ†å¸ƒ
            f.write("## ğŸ¬ è§†é¢‘ç±»å‹åˆ†å¸ƒ\n\n")
            video_types = summary.get('video_types', {})
            if video_types:
                for vtype, count in video_types.items():
                    percentage = (count / summary.get('analyzed_videos', 1)) * 100
                    f.write(f"- **{vtype}**: {count} ä¸ª ({percentage:.1f}%)\n")
            f.write("\n")
            
            # é£æ ¼ç‰¹ç‚¹
            f.write("## ğŸ¨ é£æ ¼ç‰¹ç‚¹\n\n")
            style_features = summary.get('style_features', {})
            if style_features:
                for style, count in list(style_features.items())[:5]:
                    f.write(f"- **{style}**: å‡ºç° {count} æ¬¡\n")
            f.write("\n")
            
            # é«˜é¢‘ä¸»é¢˜
            f.write("## ğŸ“Œ é«˜é¢‘ä¸»é¢˜\n\n")
            topics = summary.get('topics', {})
            if topics:
                for i, (topic, count) in enumerate(list(topics.items())[:10], 1):
                    f.write(f"{i}. **{topic}** ({count} æ¬¡)\n")
            f.write("\n")
            
            # é«˜é¢‘å…³é”®è¯
            f.write("## ğŸ”‘ é«˜é¢‘å…³é”®è¯\n\n")
            keywords = summary.get('top_keywords', [])
            if keywords:
                # æ¯è¡Œæ˜¾ç¤º 5 ä¸ªå…³é”®è¯
                for i in range(0, len(keywords), 5):
                    kw_line = keywords[i:i+5]
                    f.write(f"- {' Â· '.join(kw_line)}\n")
            f.write("\n")
            
            # æ ‡é¢˜ç‰¹å¾
            f.write("## ğŸ“ æ ‡é¢˜ç‰¹å¾\n\n")
            title_patterns = summary.get('title_patterns', {})
            if title_patterns:
                avg_length = title_patterns.get('average_length', 0)
                f.write(f"- **å¹³å‡æ ‡é¢˜é•¿åº¦**: {avg_length} å­—ç¬¦\n\n")
                
                common_starts = title_patterns.get('common_starts', {})
                if common_starts:
                    f.write("**å¸¸è§æ ‡é¢˜å¼€å¤´**:\n\n")
                    for word, count in list(common_starts.items())[:5]:
                        f.write(f"- `{word}` (ä½¿ç”¨ {count} æ¬¡)\n")
                f.write("\n")
                
                punctuation = title_patterns.get('punctuation_usage', {})
                if punctuation:
                    f.write("**æ ‡ç‚¹ç¬¦å·ä½¿ç”¨**:\n\n")
                    for punc, count in punctuation.items():
                        if count > 0:
                            f.write(f"- {punc}: {count} æ¬¡\n")
                f.write("\n")
            
            # å¸å¼•è§‚ä¼—æŠ€å·§
            f.write("## ğŸ’¡ å¸å¼•è§‚ä¼—æŠ€å·§\n\n")
            engagement = summary.get('engagement_techniques', {})
            if engagement:
                for i, (technique, count) in enumerate(list(engagement.items())[:10], 1):
                    f.write(f"{i}. **{technique}** (ä½¿ç”¨ {count} æ¬¡)\n")
            f.write("\n")
            
            # å†…å®¹ç»“æ„æ¨¡å¼
            f.write("## ğŸ“‹ å†…å®¹ç»“æ„æ¨¡å¼\n\n")
            content_patterns = summary.get('content_patterns', {})
            if content_patterns:
                for pattern, count in content_patterns.items():
                    f.write(f"- **{pattern}**: {count} ä¸ªè§†é¢‘\n")
            f.write("\n")
        
        return str(summary_file)
    
    def _generate_statistics_doc(self, output_dir: Path, channel_name: str, summary: Dict, analysis_results: List[Dict]) -> str:
        """
        ç”Ÿæˆç»Ÿè®¡æ•°æ®æ–‡æ¡£
        
        å‚æ•°:
            output_dir: è¾“å‡ºç›®å½•
            channel_name: é¢‘é“åç§°
            summary: é£æ ¼æ€»ç»“
            analysis_results: åˆ†æç»“æœåˆ—è¡¨
            
        è¿”å›:
            æ–‡æ¡£æ–‡ä»¶è·¯å¾„
        """
        stats_file = output_dir / "statistics.md"
        
        with open(stats_file, 'w', encoding='utf-8') as f:
            f.write(f"# {channel_name} - è¯¦ç»†ç»Ÿè®¡æ•°æ®\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write("---\n\n")
            
            # è§†é¢‘åˆ†æçŠ¶æ€ç»Ÿè®¡
            f.write("## ğŸ“ˆ åˆ†æçŠ¶æ€ç»Ÿè®¡\n\n")
            successful = len([r for r in analysis_results if r.get('analysis_status') == 'success'])
            failed = len([r for r in analysis_results if r.get('analysis_status') == 'failed'])
            skipped = len([r for r in analysis_results if r.get('analysis_status') == 'skipped'])
            
            f.write(f"- âœ… æˆåŠŸåˆ†æ: {successful}\n")
            f.write(f"- âŒ åˆ†æå¤±è´¥: {failed}\n")
            f.write(f"- â­ï¸ è·³è¿‡åˆ†æ: {skipped}\n\n")
            
            # è§†é¢‘ç±»å‹è¯¦ç»†ç»Ÿè®¡
            f.write("## ğŸ¬ è§†é¢‘ç±»å‹è¯¦ç»†ç»Ÿè®¡\n\n")
            video_types = summary.get('video_types', {})
            total = sum(video_types.values())
            
            f.write("| è§†é¢‘ç±»å‹ | æ•°é‡ | å æ¯” |\n")
            f.write("|---------|------|------|\n")
            for vtype, count in video_types.items():
                percentage = (count / total * 100) if total > 0 else 0
                f.write(f"| {vtype} | {count} | {percentage:.1f}% |\n")
            f.write("\n")
            
            # ä¸»é¢˜ç»Ÿè®¡
            f.write("## ğŸ“Œ ä¸»é¢˜ç»Ÿè®¡ (Top 20)\n\n")
            topics = summary.get('topics', {})
            f.write("| æ’å | ä¸»é¢˜ | å‡ºç°æ¬¡æ•° |\n")
            f.write("|------|------|----------|\n")
            for i, (topic, count) in enumerate(list(topics.items())[:20], 1):
                f.write(f"| {i} | {topic} | {count} |\n")
            f.write("\n")
            
            # é£æ ¼ç‰¹ç‚¹ç»Ÿè®¡
            f.write("## ğŸ¨ é£æ ¼ç‰¹ç‚¹ç»Ÿè®¡\n\n")
            styles = summary.get('style_features', {})
            f.write("| é£æ ¼ç‰¹ç‚¹ | å‡ºç°æ¬¡æ•° |\n")
            f.write("|----------|----------|\n")
            for style, count in styles.items():
                f.write(f"| {style} | {count} |\n")
            f.write("\n")
            
            # å…³é”®è¯ç»Ÿè®¡
            f.write("## ğŸ”‘ å…³é”®è¯ç»Ÿè®¡\n\n")
            keywords = summary.get('top_keywords', [])
            f.write("| æ’å | å…³é”®è¯ |\n")
            f.write("|------|--------|\n")
            for i, kw in enumerate(keywords, 1):
                f.write(f"| {i} | {kw} |\n")
            f.write("\n")
        
        return str(stats_file)
    
    def _generate_video_details(self, videos_dir: Path, analysis_results: List[Dict], videos_data: List[Dict]):
        """
        ç”Ÿæˆæ¯ä¸ªè§†é¢‘çš„è¯¦ç»†åˆ†ææ–‡æ¡£
        
        å‚æ•°:
            videos_dir: è§†é¢‘è¯¦æƒ…ç›®å½•
            analysis_results: åˆ†æç»“æœåˆ—è¡¨
            videos_data: åŸå§‹è§†é¢‘æ•°æ®åˆ—è¡¨
        """
        # åˆ›å»ºè§†é¢‘æ•°æ®æ˜ å°„
        video_data_map = {v.get('video_id'): v for v in videos_data}
        
        for analysis in analysis_results:
            if analysis.get('analysis_status') != 'success':
                continue
            
            video_id = analysis.get('video_id', 'unknown')
            title = analysis.get('title', 'Untitled')
            
            # æ¸…ç†æ ‡é¢˜ä½œä¸ºæ–‡ä»¶å
            safe_title = self._sanitize_filename(title)[:50]  # é™åˆ¶é•¿åº¦
            video_file = videos_dir / f"{video_id}_{safe_title}.md"
            
            with open(video_file, 'w', encoding='utf-8') as f:
                f.write(f"# {title}\n\n")
                
                # åŸºæœ¬ä¿¡æ¯
                video_data = video_data_map.get(video_id, {})
                f.write("## ğŸ“º åŸºæœ¬ä¿¡æ¯\n\n")
                f.write(f"- **è§†é¢‘ ID**: {video_id}\n")
                f.write(f"- **è§†é¢‘é“¾æ¥**: {video_data.get('url', 'N/A')}\n")
                f.write(f"- **ä¸Šä¼ æ—¶é—´**: {video_data.get('upload_date', 'N/A')}\n")
                f.write(f"- **æ—¶é•¿**: {video_data.get('duration', 0)} ç§’\n")
                f.write(f"- **è§‚çœ‹æ•°**: {video_data.get('view_count', 0):,}\n")
                f.write(f"- **ç‚¹èµæ•°**: {video_data.get('like_count', 0):,}\n\n")
                
                # å†…å®¹åˆ†æ
                f.write("## ğŸ” å†…å®¹åˆ†æ\n\n")
                f.write(f"- **è§†é¢‘ç±»å‹**: {analysis.get('video_type', 'N/A')}\n")
                f.write(f"- **è¯­è¨€é£æ ¼**: {analysis.get('style', 'N/A')}\n")
                f.write(f"- **è¯­æ°”**: {analysis.get('tone', 'N/A')}\n")
                f.write(f"- **ç›®æ ‡å—ä¼—**: {analysis.get('target_audience', 'N/A')}\n\n")
                
                # ä¸»é¢˜å’Œå…³é”®è¯
                f.write("### ä¸»è¦ä¸»é¢˜\n\n")
                topics = analysis.get('topics', [])
                if topics:
                    for topic in topics:
                        f.write(f"- {topic}\n")
                f.write("\n")
                
                f.write("### å…³é”®è¯\n\n")
                keywords = analysis.get('keywords', [])
                if keywords:
                    f.write(f"{' Â· '.join(keywords[:10])}\n\n")
                
                # å†…å®¹ç»“æ„
                f.write("### å†…å®¹ç»“æ„\n\n")
                f.write(f"{analysis.get('content_structure', 'N/A')}\n\n")
                
                # æ ¸å¿ƒè¦ç‚¹
                f.write("### æ ¸å¿ƒè¦ç‚¹\n\n")
                key_points = analysis.get('key_points', [])
                if key_points:
                    for i, point in enumerate(key_points, 1):
                        f.write(f"{i}. {point}\n")
                f.write("\n")
                
                # å¸å¼•æŠ€å·§
                f.write("### å¸å¼•è§‚ä¼—æŠ€å·§\n\n")
                techniques = analysis.get('engagement_techniques', [])
                if techniques:
                    for tech in techniques:
                        f.write(f"- {tech}\n")
                f.write("\n")
                
                # å­—å¹•å†…å®¹(èŠ‚é€‰)
                subtitle_text = video_data.get('subtitle_text', '')
                if subtitle_text:
                    f.write("## ğŸ“ å­—å¹•å†…å®¹èŠ‚é€‰\n\n")
                    f.write("```\n")
                    # åªæ˜¾ç¤ºå‰ 500 å­—ç¬¦
                    preview = subtitle_text[:500]
                    if len(subtitle_text) > 500:
                        preview += "..."
                    f.write(preview)
                    f.write("\n```\n\n")
    
    def _generate_learning_guide(self, output_dir: Path, channel_name: str, summary: Dict) -> str:
        """
        ç”Ÿæˆå­¦ä¹ æŒ‡å—
        
        å‚æ•°:
            output_dir: è¾“å‡ºç›®å½•
            channel_name: é¢‘é“åç§°
            summary: é£æ ¼æ€»ç»“
            
        è¿”å›:
            æ–‡æ¡£æ–‡ä»¶è·¯å¾„
        """
        guide_file = output_dir / "learning_guide.md"
        
        with open(guide_file, 'w', encoding='utf-8') as f:
            f.write(f"# {channel_name} - å­¦ä¹ ä¸æ¨¡ä»¿æŒ‡å—\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write("---\n\n")
            
            f.write("## ğŸ¯ æ ¸å¿ƒç‰¹å¾æ€»ç»“\n\n")
            
            # å†…å®¹å®šä½
            f.write("### 1. å†…å®¹å®šä½\n\n")
            primary_type = summary.get('primary_type', 'æœªçŸ¥')
            f.write(f"è¯¥é¢‘é“ä¸»è¦åˆ¶ä½œã€{primary_type}ã€‘ç±»å‹çš„è§†é¢‘,")
            primary_audience = summary.get('primary_audience', 'å¤§ä¼—')
            f.write(f"ç›®æ ‡å—ä¼—ä¸ºã€{primary_audience}ã€‘ã€‚\n\n")
            
            # é£æ ¼ç‰¹ç‚¹
            f.write("### 2. é£æ ¼ç‰¹ç‚¹\n\n")
            style_features = summary.get('style_features', {})
            if style_features:
                top_styles = list(style_features.keys())[:3]
                f.write("è¯¥é¢‘é“çš„è¯­è¨€é£æ ¼ç‰¹ç‚¹:\n\n")
                for style in top_styles:
                    f.write(f"- {style}\n")
            f.write("\n")
            
            # å†…å®¹ä¸»é¢˜
            f.write("### 3. å†…å®¹ä¸»é¢˜\n\n")
            topics = summary.get('topics', {})
            if topics:
                f.write("é¢‘é“ç»å¸¸æ¶‰åŠçš„ä¸»é¢˜:\n\n")
                for topic in list(topics.keys())[:10]:
                    f.write(f"- {topic}\n")
            f.write("\n")
            
            # æ ‡é¢˜æŠ€å·§
            f.write("### 4. æ ‡é¢˜æŠ€å·§\n\n")
            title_patterns = summary.get('title_patterns', {})
            if title_patterns:
                avg_length = title_patterns.get('average_length', 0)
                f.write(f"- æ ‡é¢˜å¹³å‡é•¿åº¦: {avg_length} å­—ç¬¦\n")
                
                common_starts = title_patterns.get('common_starts', {})
                if common_starts:
                    f.write("- å¸¸ç”¨å¼€å¤´è¯: ")
                    f.write(", ".join(list(common_starts.keys())[:5]))
                    f.write("\n")
                
                punctuation = title_patterns.get('punctuation_usage', {})
                high_usage_punct = [p for p, c in punctuation.items() if c > 3]
                if high_usage_punct:
                    f.write(f"- å¸¸ç”¨æ ‡ç‚¹: {', '.join(high_usage_punct)}\n")
            f.write("\n")
            
            # å¸å¼•æŠ€å·§
            f.write("### 5. å¸å¼•è§‚ä¼—æŠ€å·§\n\n")
            engagement = summary.get('engagement_techniques', {})
            if engagement:
                f.write("è¯¥é¢‘é“å¸¸ç”¨çš„å¸å¼•è§‚ä¼—æŠ€å·§:\n\n")
                for i, technique in enumerate(list(engagement.keys())[:8], 1):
                    f.write(f"{i}. {technique}\n")
            f.write("\n")
            
            # æ¨¡ä»¿å»ºè®®
            f.write("## ğŸ’¡ æ¨¡ä»¿å»ºè®®\n\n")
            
            f.write("### å†…å®¹åˆ›ä½œæ–¹å‘\n\n")
            f.write(f"1. **å®šä½æ˜ç¡®**: èšç„¦äºã€{primary_type}ã€‘ç±»å‹å†…å®¹\n")
            f.write(f"2. **å—ä¼—å®šä½**: é’ˆå¯¹ã€{primary_audience}ã€‘åˆ›ä½œå†…å®¹\n")
            if topics:
                top_topics = list(topics.keys())[:5]
                f.write(f"3. **ä¸»é¢˜é€‰æ‹©**: å›´ç»• {', '.join(top_topics)} ç­‰ä¸»é¢˜å±•å¼€\n")
            f.write("\n")
            
            f.write("### é£æ ¼å¡‘é€ \n\n")
            if style_features:
                for i, style in enumerate(list(style_features.keys())[:3], 1):
                    f.write(f"{i}. ä¿æŒã€{style}ã€‘çš„è¡¨è¾¾æ–¹å¼\n")
            f.write("\n")
            
            f.write("### æ ‡é¢˜æ’°å†™\n\n")
            if title_patterns:
                avg_length = title_patterns.get('average_length', 0)
                f.write(f"1. æ ‡é¢˜é•¿åº¦æ§åˆ¶åœ¨ {int(avg_length * 0.8)}-{int(avg_length * 1.2)} å­—ç¬¦å·¦å³\n")
                
                common_starts = title_patterns.get('common_starts', {})
                if common_starts:
                    f.write(f"2. å¯ä»¥å°è¯•ä½¿ç”¨ã€Œ{list(common_starts.keys())[0]}ã€ç­‰å¼€å¤´\n")
                
                f.write("3. å–„ç”¨æ ‡ç‚¹ç¬¦å·å¢å¼ºå¸å¼•åŠ›\n")
            f.write("\n")
            
            f.write("### å†…å®¹æŠ€å·§\n\n")
            if engagement:
                techniques = list(engagement.keys())[:5]
                for i, tech in enumerate(techniques, 1):
                    f.write(f"{i}. {tech}\n")
            f.write("\n")
            
            # å…³é”®æˆåŠŸå› ç´ 
            f.write("## ğŸ”‘ å…³é”®æˆåŠŸå› ç´ \n\n")
            f.write("åŸºäºåˆ†æ,è¯¥é¢‘é“çš„æˆåŠŸå…³é”®å› ç´ å¯èƒ½åŒ…æ‹¬:\n\n")
            f.write("1. **ä¸€è‡´çš„é£æ ¼å®šä½**: ä¿æŒç»Ÿä¸€çš„å†…å®¹ç±»å‹å’Œé£æ ¼\n")
            f.write("2. **æ˜ç¡®çš„å—ä¼—ç¾¤ä½“**: äº†è§£å¹¶æœåŠ¡å¥½ç›®æ ‡å—ä¼—\n")
            f.write("3. **æŒç»­çš„ä¸»é¢˜æ·±è€•**: åœ¨ç‰¹å®šé¢†åŸŸå»ºç«‹ä¸“ä¸šåº¦\n")
            if engagement:
                f.write("4. **å¤šæ ·çš„äº’åŠ¨æŠ€å·§**: è¿ç”¨å¤šç§æ–¹å¼å¸å¼•å’Œç•™ä½è§‚ä¼—\n")
            f.write("\n")
            
            f.write("---\n\n")
            f.write("**æ³¨**: ä»¥ä¸Šåˆ†æåŸºäºè§†é¢‘å†…å®¹çš„å®¢è§‚æ•°æ®,æ¨¡ä»¿æ—¶è¯·ç»“åˆè‡ªèº«ç‰¹ç‚¹,å½¢æˆç‹¬ç‰¹é£æ ¼ã€‚\n")
        
        return str(guide_file)
    
    def _generate_wordcloud_image(self, output_dir: Path, summary: Dict) -> str:
        """
        ç”Ÿæˆè¯äº‘å›¾
        
        å‚æ•°:
            output_dir: è¾“å‡ºç›®å½•
            summary: é£æ ¼æ€»ç»“
            
        è¿”å›:
            è¯äº‘å›¾æ–‡ä»¶è·¯å¾„
        """
        try:
            from wordcloud import WordCloud
            import matplotlib.pyplot as plt
            
            # è·å–å…³é”®è¯
            keywords = summary.get('top_keywords', [])
            if not keywords:
                return None
            
            # æ„å»ºè¯é¢‘å­—å…¸
            topics = summary.get('topics', {})
            word_freq = {word: freq for word, freq in topics.items()}
            
            # å¦‚æœè¯é¢‘ä¸ºç©º,ä½¿ç”¨å…³é”®è¯åˆ—è¡¨
            if not word_freq:
                word_freq = {kw: len(keywords) - i for i, kw in enumerate(keywords)}
            
            # ç”Ÿæˆè¯äº‘
            wordcloud = WordCloud(
                width=1200,
                height=600,
                background_color='white',
                font_path=None,  # è‡ªåŠ¨é€‰æ‹©å­—ä½“
                max_words=100,
                relative_scaling=0.5,
                colormap='viridis'
            ).generate_from_frequencies(word_freq)
            
            # ä¿å­˜å›¾ç‰‡
            wordcloud_file = output_dir / "wordcloud.png"
            plt.figure(figsize=(12, 6))
            plt.imshow(wordcloud, interpolation='bilinear')
            plt.axis('off')
            plt.tight_layout(pad=0)
            plt.savefig(wordcloud_file, dpi=150, bbox_inches='tight')
            plt.close()
            
            return str(wordcloud_file)
            
        except Exception as e:
            self.logger.warning(f"ç”Ÿæˆè¯äº‘å›¾å¤±è´¥: {str(e)}")
            return None

